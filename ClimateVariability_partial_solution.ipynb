{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": [
        "-8p5cbyxUtnC"
      ],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/davidnoone/GEOPHYS_NOTEBOOKS/blob/main/ClimateVariability_partial_solution.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Climate variability - solution"
      ],
      "metadata": {
        "id": "brqRz1GoT9y7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Obtain the data files\n",
        "!test ! -f meiv2.data && wget -O meiv2.data http://kete.rangi.cloud.edu.au/u/dcn/meiv2.data\n",
        "!test ! -f ERA5_monthly_surface.nc && wget -O ERA5_monthly_surface.nc http://kete.rangi.cloud.edu.au/u/dcn/ERA5_monthly_surface.nc\n",
        "!test ! -f ERA5_monthly_4layer.nc && wget -O ERA5_monthly_4layer.nc http://kete.rangi.cloud.edu.au/u/dcn/ERA5_monthly_4layer.nc"
      ],
      "metadata": {
        "id": "5mU8ezixUZ0c"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "a3ATRaKAT9Mq"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "from scipy import stats\n",
        "from scipy import linalg\n",
        "import matplotlib\n",
        "import matplotlib.pyplot as plt\n",
        "try:\n",
        "  import netCDF4 as nc\n",
        "except:\n",
        "  !pip install netCDF4\n",
        "  import netCDF4 as nc\n",
        "\n",
        "from datetime import datetime, timedelta\n",
        "\n",
        "from google.colab import files"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#ncdump\n",
        "ncdump is a little (unix) tool for showing the contents of a netcdf file.\n",
        "Here is a python function which is similar. "
      ],
      "metadata": {
        "id": "-8p5cbyxUtnC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Example: show contents of netcdf file (like \"ncdump\" on )\n",
        "def ncdump(nc_fid, verb=True):\n",
        "    '''\n",
        "    ncdump outputs dimensions, variables and their attribute information.\n",
        "    The information is similar to that of NCAR's ncdump utility.\n",
        "    ncdump requires a valid instance of Dataset.\n",
        "\n",
        "    Parameters\n",
        "    ----------\n",
        "    nc_fid : netCDF4.Dataset\n",
        "        A netCDF4 dateset object\n",
        "    verb : Boolean\n",
        "        whether or not nc_attrs, nc_dims, and nc_vars are printed\n",
        "\n",
        "    Returns\n",
        "    -------\n",
        "    nc_attrs : list\n",
        "        A Python list of the NetCDF file global attributes\n",
        "    nc_dims : list\n",
        "        A Python list of the NetCDF file dimensions\n",
        "    nc_vars : list\n",
        "        A Python list of the NetCDF file variables\n",
        "    '''\n",
        "    def print_ncattr(key):\n",
        "        \"\"\"\n",
        "        Prints the NetCDF file attributes for a given key\n",
        "\n",
        "        Parameters\n",
        "        ----------\n",
        "        key : unicode\n",
        "            a valid netCDF4.Dataset.variables key\n",
        "        \"\"\"\n",
        "        try:\n",
        "            print(\"\\t\\ttype:\", repr(nc_fid.variables[key].dtype))\n",
        "            for ncattr in nc_fid.variables[key].ncattrs():\n",
        "                print('\\t\\t%s:' % ncattr,\\\n",
        "                      repr(nc_fid.variables[key].getncattr(ncattr)))\n",
        "        except KeyError:\n",
        "            print(\"\\t\\tWARNING: %s does not contain variable attributes\" % key)\n",
        "\n",
        "    # NetCDF global attributes\n",
        "    nc_attrs = nc_fid.ncattrs()\n",
        "    if verb:\n",
        "        print(\"NetCDF Global Attributes:\")\n",
        "        for nc_attr in nc_attrs:\n",
        "            print('\\t%s:' % nc_attr, repr(nc_fid.getncattr(nc_attr)))\n",
        "    nc_dims = [dim for dim in nc_fid.dimensions]  # list of nc dimensions\n",
        "    # Dimension shape information.\n",
        "    if verb:\n",
        "        print(\"NetCDF dimension information:\")\n",
        "        for dim in nc_dims:\n",
        "            print(\"\\tName:\", dim )\n",
        "            print(\"\\t\\tsize:\", len(nc_fid.dimensions[dim]))\n",
        "            print_ncattr(dim)\n",
        "    # Variable information.\n",
        "    nc_vars = [var for var in nc_fid.variables]  # list of nc variables\n",
        "    if verb:\n",
        "        print(\"NetCDF variable information:\")\n",
        "        for var in nc_vars:\n",
        "            if var not in nc_dims:\n",
        "                print('\\tName:', var)\n",
        "                print(\"\\t\\tdimensions:\", nc_fid.variables[var].dimensions)\n",
        "                print(\"\\t\\tsize:\", nc_fid.variables[var].size)\n",
        "                print_ncattr(var)\n",
        "    return nc_attrs, nc_dims, nc_vars"
      ],
      "metadata": {
        "id": "BpVN_sBSveCe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Open and read the data"
      ],
      "metadata": {
        "id": "yUjvBznj1IA-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# open and read the netcdf files\n",
        "file_erap = 'ERA5_monthly_4layer.nc'\n",
        "file_eras = 'ERA5_monthly_surface.nc'\n",
        "\n",
        "\n",
        "with nc.Dataset(file_eras,'r') as fid:\n",
        "    #ncdump(fid,verb=True)\n",
        "    time = fid.variables['time'][:]\n",
        "    lons = fid.variables['longitude'][:]\n",
        "    lats = fid.variables['latitude'][:]\n",
        "    data = fid.variables['t2m'][:,:,:]\n",
        "\n",
        "# do some masking\n",
        "nn =20\n",
        "data[:,:nn,:] = 0\n",
        "data[:,-nn:,:] = 0\n",
        "\n",
        "#nlon = np.size(lons)\n",
        "#nlat = np.size(lats)\n",
        "ntime, nlat, nlon = np.shape(data)\n",
        "print ('nlon:',nlon,'nlat:',nlat, 'ntime:',ntime)\n",
        "\n",
        "\n",
        "# Example to read time from file:\n",
        "#Set \"zero\" as per netcdf time units\n",
        "dt_zero = datetime.strptime(\"1900-01-01 00:00:00\",\"%Y-%m-%d %H:%M:%S\")\n",
        "year = []\n",
        "for h in time:\n",
        "    dt = dt_zero + timedelta(hours=float(h))\n",
        "    year.append(dt.year)\n",
        "print('YEAR:',year)\n"
      ],
      "metadata": {
        "id": "B1x65qAJUs_9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# open and read the MEI data file\n",
        "file_mei = 'meiv2.data'\n",
        "columns = ['Year', 'Jan', 'Feb', 'Mar', 'Apr', 'May', \n",
        "                   'Jun', 'Jul', 'Aug', 'Sep', 'Oct', 'Nov', 'Dec']\n",
        "df = pd.read_csv(file_mei, sep='\\s+', header=None, names=columns)\n"
      ],
      "metadata": {
        "id": "EosyGkNXUGDx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Example function to perform PCA"
      ],
      "metadata": {
        "id": "iiBAIMy0UzRx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def pca_eig(Z):\n",
        "  \"\"\" \n",
        "      A simple principle component analysis with eigenvalue decomposition\n",
        "      Z(x,t) = sum_M a(t)e(x)\n",
        "\n",
        "      Z MUST BE PASSED IN AS CENTRED (mean remove)\n",
        "\n",
        "      with e the eiegen vectors and a the temporal loadings\n",
        "      Notice that if nspace > ntime is is MUCH better (faster)\n",
        "      Here, we do this as a simple transpose, and leave it\n",
        "      to the calling code to figure out how to manage the rescaling.\n",
        "  \n",
        "  \"\"\"\n",
        "  # want to work in the smallest dimensions\n",
        "  transpose = False\n",
        "  ntime, nspace = np.shape(Z)\n",
        "  if (ntime < nspace):\n",
        "      print('PCA: doing transpose to speed up calculation')\n",
        "      Z = np.transpose(Z)\n",
        "      transpose = True\n",
        "\n",
        "  # Compute the covariance matrix\n",
        "  covariance_matrix = np.cov(Z, rowvar=False)\n",
        "  print('Covaraince matrix:',np.shape(covariance_matrix))\n",
        "\n",
        "  # Calulate the eigen values/vectors of the covariance\n",
        "  eigenvalues, eigenvectors = linalg.eigh(covariance_matrix)\n",
        "\n",
        "  # Sort these, since python (actually LAPACK!) doesn't do it\n",
        "  idx = np.argsort(eigenvalues)[::-1]\n",
        "  eigenvalues  = eigenvalues[idx]\n",
        "  eigenvectors = eigenvectors[:, idx]\n",
        "\n",
        "  # Project the data onto the new eigen space to obtain the time factors\n",
        "  principal_components = np.dot(Z, eigenvectors)\n",
        "\n",
        "  # If transposed input, transpose output\n",
        "  if (transpose):\n",
        "      eigenvectors         = np.transpose(eigenvectors)\n",
        "      principal_components = np.transpose(principal_components)\n",
        "      return eigenvalues, principal_components, eigenvectors  # return in reverse order\n",
        "\n",
        "  return eigenvalues, eigenvectors, principal_components"
      ],
      "metadata": {
        "id": "G0PzU85NUywq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# This version using SVD PCA; Z is centered on input\n",
        "def pca_svd(Z):\n",
        "    \"\"\" \n",
        "       Performs principal component analysis using SVD of the input \"Z\"\n",
        "       data matrix. Z should be \"centered\" on input. (Z = data - mean(data))\n",
        "\n",
        "       Z = U s V.T\n",
        "\n",
        "      Equivalence with the PCA problem notes\n",
        "      Z = A E\n",
        "\n",
        "      With variance: \n",
        "           lambda = s^2/(n-1)\n",
        "           loadings: US\n",
        "           eigenfunctions: V\n",
        "\n",
        "      if ntime < nspace the loadings\n",
        "\n",
        "    \"\"\"\n",
        "\n",
        "    ntime, mspace = np.shape(Z)\n",
        "    print ('SVD Z shape:',ntime, mspace)\n",
        "\n",
        "    # Perform Singular Value Decomposition (SVD) on the centered data\n",
        "    U, s, Vt = np.linalg.svd(Z, full_matrices=False)\n",
        "\n",
        "    # Compute loadings\n",
        "    US = np.dot(U, np.diag(s))\n",
        "\n",
        "    # Compute eigenvalues from the singular values\n",
        "    eigenvalues = s ** 2 / (ntime - 1)\n",
        "\n",
        "    # Compute the principal components by multiplying the centered data with eigenvectors\n",
        "    #principal_components = np.dot(Z, Vt.T)\n",
        "    #principal_components = principal_components.T\n",
        "\n",
        "    # depending on if space or time is larger, \n",
        "    # time > space\n",
        "    #eigenvectors = Vt.T\n",
        "\n",
        "    # space > time\n",
        "    eigenvectors = Vt\n",
        "    principal_components = US.T\n",
        "    \n",
        "\n",
        "    # normalize\n",
        "    nnorm = np.sqrt(mspace)\n",
        "    eigenvectors = eigenvectors*nnorm\n",
        "    principal_components = principal_components/nnorm\n",
        "\n",
        "    return eigenvalues, eigenvectors, principal_components"
      ],
      "metadata": {
        "id": "n9KkO9IMQwyj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Part 1: regression maps"
      ],
      "metadata": {
        "id": "JdxeQKunXNbK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "lat_pnt = -36.8509\n",
        "lon_pnt = 174.7645\n",
        "\n",
        "ipnt = np.argmin(np.abs(lons - lon_pnt))\n",
        "jpnt = np.argmin(np.abs(lats - lat_pnt))\n",
        "print('Clostest point:',lons[ipnt],lats[jpnt])\n"
      ],
      "metadata": {
        "id": "zZYF6wDlXNAL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Part 2: patterns of variability"
      ],
      "metadata": {
        "id": "nU5FvFBfXQE1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#\n",
        "print('Doing principal component analysis')\n",
        "z_data = data.reshape(ntime,nlat*nlon)\n",
        "z_mean = np.mean(z_data,axis=0)\n",
        "print('Zmean:',np.shape(z_mean))\n",
        "for n in range(ntime):\n",
        "    z_data[n,:] = z_data[n,:] - z_mean[:]\n",
        "\n",
        "data_variance = np.sum(np.var(z_data, axis=1))\n",
        "\n",
        "# Perform the PCA: notice order of PC and Evectors when working in time domain\n",
        "eval, evec, afac = pca_eig(z_data) \n",
        "\n",
        "print('RAW: vals:',np.shape(eval),'vec:',np.shape(evec),'pcs:',np.shape(afac))\n",
        "evec = evec.reshape(ntime, nlat, nlon)\n",
        "\n",
        "# Eigen values are proportional to variance, so figure out the fractional vaianc explained\n",
        "total_variance = np.sum(eval)\n",
        "variance_explained = eval/total_variance\n",
        "\n",
        "print('VARIANCE (original, from evals):',data_variance, total_variance)\n",
        "#print('vals:',np.shape(eval),'vec:',np.shape(evec),'pcs:',np.shape(afac))\n",
        "#\n",
        "# Let;s check the eigen vectors are \"unitless\"\n",
        "print('EIGENVECTOR MAGNITUDE:',np.linalg.norm(evec, axis=1))\n",
        "print('P COMPONENT MAGNITUDE:',np.linalg.norm(afac, axis=0))\n",
        "print('EIGENVALUES          :',eval)\n",
        "print('Variance_explained   :',variance_explained*100)\n",
        "\n",
        "\n",
        "# Some plots\n",
        "num = 1\n",
        "fig, ax = plt.subplots(3, 2, figsize=(10, 12))\n",
        "\n",
        "cf = ax[0,0].contourf(lons,lats,evec[num,:,:])\n",
        "cb = plt.colorbar(cf)\n",
        "ax[1,0].plot(year,afac[num,:])\n",
        "ax[2,0].plot(variance_explained*100)\n",
        "\n",
        "#\n",
        "# Do it again with SVD \n",
        "#\n",
        "eval, evec, afac = pca_svd(z_data)\n",
        "\n",
        "evec = evec.reshape(ntime, nlat, nlon)\n",
        "total_variance = np.sum(eval)\n",
        "variance_explained = eval/total_variance\n",
        "\n",
        "print('SVD')\n",
        "print('VARIANCE (original, from evals):',data_variance, total_variance)\n",
        "print('EIGENVECTOR MAGNITUDE:',np.linalg.norm(evec, axis=1))\n",
        "print('P COMPONENT MAGNITUDE:',np.linalg.norm(afac, axis=0))\n",
        "print('EIGENVALUES          :',eval)\n",
        "print('Variance_explained   :',variance_explained*100)\n",
        "\n",
        "\n",
        "\n",
        "cf = ax[0,1].contourf(lons,lats,evec[num,:,:])\n",
        "cb = plt.colorbar(cf)\n",
        "ax[1,1].plot(year,afac[num,:])\n",
        "ax[2,1].plot(variance_explained*100)\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "XkJK7fqTXPRL"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}